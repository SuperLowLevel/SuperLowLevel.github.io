---
title:  'Activating More Pixels in Image Super-Resolution Transformer'  #  Paper title, covered by ''
teser: hat.png
type:   paper
pro_type: Image Super-Resolution
layout: post  #  Do not change this
date:   2022-05-03 11:59:59 +0800  # paper pub data, only change year and month according to this format
author: Xiangyu Chen, Xintao Wang, Jiantao Zhou, Chao Dong # authors information
venue:  Arxiv, 2022 #Where it be, ICCV and CVPR remove IEEE Conference on,
year:   2022  # paper year, number
month:  May # paper month, full name
projectPage: None  # If has project page, link here, otherwise None
supplemental : None
data: None  # If has data, post data link here, otherwise None
code: https://github.com/chxy95/HAT  # If has data, post code link here, otherwise None
paperLink: https://arxiv.org/pdf/2205.03409.pdf # post paper pdf link here
---

Transformer-based methods have shown impressive performance in low-level vision tasks, such as image super-resolution. However, we find that these networks can only utilize a limited spatial range of input information through attribution analysis. This implies that the potential of Transformer is still not fully exploited in existing networks. In order to activate more input pixels for reconstruction, we propose a novel Hybrid Attention Transformer (HAT). It combines channel attention and self-attention schemes, thus making use of their complementary advantages. Moreover, to better aggregate the cross-window information, we introduce an overlapping cross-attention module to enhance the interaction between neighboring window features. In the training stage, we additionally propose a same-task pre-training strategy to bring further improvement. Extensive experiments show the effectiveness of the proposed modules, and the overall method significantly outperforms the state-of-the-art methods by more than 1dB.
