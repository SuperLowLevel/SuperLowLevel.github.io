---
title:  'Temporally Consistent Video Colorization with Deep Feature Propagation and Self-regularization Learning'  #  Paper title, covered by ''
teser: TCVC.png
type:  paper
pro_type: Video Colorization
layout: post  #  Do not change this
date:   2022-10-10 11:59:59 +0800  # paper pub data, only change year and month according to this format
author: Yihao Liu, Hengyuan Zhao, Kelvin C.K. Chan, Xintao Wang, Chen Change Loy, Yu Qiao, Chao Dong
venue:  Computational Visual Media (CVMJ), 2023 #Where it be, ICCV and CVPR remove IEEE Conference on,
year:   2023  # paper year, number
month:  March # paper month, full name
projectPage: None  # If has project page, link here, otherwise None
supplemental : None
data: None  # If has data, post data link here, otherwise None
code: https://github.com/lyh-18/TCVC-Temporally-Consistent-Video-Colorization  # If has data, post code link here, otherwise None
paperLink: https://arxiv.org/abs/2110.04562 # post paper pdf link here
---
Video colorization is a challenging and highly ill-posed problem. Although recent years have witnessed remarkable
progress in single image colorization, there is relatively less research effort on video colorization and existing methods always suffer
from severe flickering artifacts (temporal inconsistency) or unsatisfying colorization performance. We address this problem from a new
perspective, by jointly considering colorization and temporal consistency in a unified framework. Specifically, we propose a novel
temporally consistent video colorization framework (TCVC). TCVC effectively propagates frame-level deep features in a bidirectional
way to enhance the temporal consistency of colorization. Furthermore, TCVC introduces a self-regularization learning (SRL) scheme
to minimize the prediction difference obtained with different time steps. SRL does not require any ground-truth color videos for training
and can further improve temporal consistency. Experiments demonstrate that our method can not only obtain visually pleasing
colorized video, but also achieve clearly better temporal consistency than state-of-the-art methods.